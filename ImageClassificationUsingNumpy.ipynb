{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ImageClassificationUsingNumpy.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xsbgYUL21qS6","colab_type":"code","colab":{}},"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","import cv2\n","import glob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbVefGJbYd4V","colab_type":"code","outputId":"1771af18-90e2-4a46-b5f8-55725be3e186","executionInfo":{"status":"ok","timestamp":1570941141895,"user_tz":-330,"elapsed":26496,"user":{"displayName":"Kolli Koushik","photoUrl":"","userId":"03448954236804754409"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["data_dir = r'dataset/dataset/'\n","classes = ['broadleaf', 'grass', 'soil', 'soybean'] \n","\n","num_file = 1100 \n","all_files = [] \n","num_data =num_file*len(classes)\n","Y = np.zeros(num_data)\n","\n","\n","for i, cls in enumerate(classes):\n","    all_files += [f for f in glob.glob(data_dir+cls+'/*.tif')][:num_file]\n","    Y[i*num_file:(i+1)*num_file] = i # label all classes with int [0.. len(classes)]\n","\n","    \n","# Image dimension\n","im_width = 200\n","im_height = 200 \n","im_channel = 3\n","dim = im_width * im_height * im_channel\n","\n","X = np.ndarray(shape=(num_data, im_width, im_height, im_channel), dtype=np.float64)\n","\n","for idx, file in enumerate(all_files):\n","    X[idx] = cv2.resize(cv2.imread(file), (im_width, im_height))\n","\n","X_train = np.empty(shape=(4000,im_width, im_height, im_channel), dtype=np.float64)\n","X_val = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.float64)\n","X_test = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.float64)\n","\n","y_train = np.empty(4000)\n","y_val = np.empty(200)\n","y_test = np.empty(200) \n","\n","for i, cls in enumerate(classes): \n","    X_test[50*i:50*(i+1)] = X[np.where(Y == i)[0][:50]]\n","    X_val[50*i:50*(i+1)] = X[np.where(Y == i)[0][50:100]]\n","    X_train[1000*i:1000*(i+1)] = X[np.where(Y == i)[0][100:]]\n","    \n","    y_test[50*i:50*(i+1)] = i\n","    y_val[50*i:50*(i+1)] = i\n","    y_train[1000*i:1000*(i+1)] = i\n","    \n","del Y \n","del X\n","\n","# Extract features \n","#Shuffle training index\n","train_idxs = np.random.permutation(X_train.shape[0])\n","y_train  = y_train[train_idxs].astype(int)\n","X_train = X_train[train_idxs]\n","\n","X_train = np.reshape(X_train, (X_train.shape[0], -1)).astype('float64')\n","X_test = np.reshape(X_test, (X_test.shape[0], -1)).astype('float64')\n","X_val = np.reshape(X_val, (X_val.shape[0], -1)).astype('float64')\n","\n","X_tiny = X_train[100:110].astype('float64')\n","y_tiny = y_train[100:110].astype(int)\n","num_dev = 500\n","\n","X_dev = X_train[0:num_dev].astype('float64')\n","y_dev = y_train[0:num_dev].astype(int)\n","print(\"X_train shape\", X_train.shape, \"| y_train shape:\", y_train.shape)\n","print(\"X_test shape\", X_test.shape, \"| y_test shape:\", y_test.shape)\n","print(\"X_val shape\", X_val.shape, \"| y_val shape:\", y_val.shape)\n","print(\"X_dev shape\", X_dev.shape, \"| y_dev shape:\", y_dev.shape)\n","print(\"X_tiny shape\", X_tiny.shape, \"| y_tiny shape:\", y_tiny.shape)\n","\n","#Subtract out the mean image \n","#first: compute the mean image\n","# mean_image = np.mean(X_train, axis=0) #axis=0. stack horizontally\n","mean_image = 128\n","#Second subtract the mean image from train and test data \n","X_train -= mean_image\n","X_val -= mean_image \n","X_test -= mean_image\n","X_dev -= mean_image\n","X_tiny -= mean_image\n","\n","#Third append the bias dimension using linear algebra trick\n","#Not for net\n","# X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","# X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","# X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","# X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n","# X_tiny = np.hstack([X_tiny, np.ones((X_tiny.shape[0], 1))])\n","\n","print('=====STACK BIAS term=====')\n","print(\"X_train shape\", X_train.shape)\n","print(\"X_test shape\", X_test.shape)\n","print(\"X_val shape\", X_val.shape)\n","print(\"X_dev shape\", X_dev.shape)\n","print(\"X_tiny shape\", X_tiny.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["X_train shape (4000, 120000) | y_train shape: (4000,)\n","X_test shape (200, 120000) | y_test shape: (200,)\n","X_val shape (200, 120000) | y_val shape: (200,)\n","X_dev shape (500, 120000) | y_dev shape: (500,)\n","X_tiny shape (10, 120000) | y_tiny shape: (10,)\n","=====STACK BIAS term=====\n","X_train shape (4000, 120000)\n","X_test shape (200, 120000)\n","X_val shape (200, 120000)\n","X_dev shape (500, 120000)\n","X_tiny shape (10, 120000)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EJMP1jC4YfeN","colab_type":"code","colab":{}},"source":["class TwoLayerNet():\n","    def __init__(self, input_size, hidden_size, output_size, std= 1e-4):\n","        '''\n","        std: weight initialization term\n","        W1: first layer weight, shape(D x H) \n","        W2: second layer weight shape(H x C) \n","        C: num_classes(output_size) , H: hidden_size, D: data_dim(input_size) \n","        '''\n","        self.params = {}\n","        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","        \n","    def loss(self, X, y = None, reg=0.0):\n","        '''\n","        reg: regularization strength\n","        X: ndarray shape(N x C). N: num of data \n","        y: vector of training label\n","        '''\n","        #DEfine relu activation function \n","        relu = lambda x:np.maximum(0,x)\n","\n","        #unpack\n","        W1, b1 = self.params['W1'], self.params['b1']\n","        W2, b2 = self.params['W2'], self.params['b2']\n","        N, D = X.shape\n","\n","        #Forward prop\n","        layer1 = relu(X.dot(W1) + b1)  #(N,D) x (D,H) = (N,H)\n","        scores = layer1.dot(W2) + b2\n","\n","        #if target is not given then jump out \n","        if(y is None): \n","            return scores\n","\n","        #compute the loss \n","        ##Normalization trick to prevent overflow when compute exp \n","        scores -= scores.max()#stack vertically\n","\n","        scores = np.exp(scores)\n","        scores_sumexp = np.sum(scores, axis=1)#stack vertically\n","\n","        ##Nomalize all score \n","        softmax = scores / scores_sumexp.reshape(N,1)  #Shape: (N, C)\n","        #total loss of all training. -log of all correct score\n","        loss =  (-1.0) * np.sum(np.log(softmax[range(N),y]))\n","\n","        ##Normalize the loss and add regularization strength \n","        loss /= N \n","        loss += reg * np.sum(W1 * W1) \n","        loss += reg * np.sum(W2 * W2) \n","\n","        #Backward pass on the net \n","        grads = {}\n","\n","        correct_class_scores = scores[range(N), y]\n","        softmax[range(N), y] = (-1.0) * (scores_sumexp - correct_class_scores)/scores_sumexp\n","        softmax /= N\n","\n","\n","        #Want to find dW2(dL/dW2)\n","        # Derivation: dL/dW2 = dL/dscore * dscore/dW2(chain rule)\n","        #dL/dscore = softmax since L(score) = softmax(variable)\n","        #dscore/dW2 = relu_(hidden layer output)\n","        grads['W2'] = layer1.T.dot(softmax)\n","        grads['b2'] = np.sum(softmax, axis=0)#stack horizontally\n","        grads['W2'] += reg * 2 * W2\n","\n","        #dL/dW1 = dL/dscore * dscore/drelu(layler1) * drelu(layer1)/dW1 \n","        #dL/dW1 = dW1 = softmax * W2 * X \n","        hidden = softmax.dot(W2.T)\n","\n","        #derivative of a max gate\n","        #Intuition: in forward pass if neuron didn't fire that mean. the derivative of that neuron \n","        # is 0. This might be bad since this will kill gradient. \n","        hidden[layer1 == 0] = 0 \n","\n","        grads['W1'] = X.T.dot(hidden) \n","        grads['b1'] = np.sum(hidden, axis=0) #stack horizontally \n","        grads['W1'] += reg * 2 * W1\n","\n","        return loss, grads\n","\n","    def train(self, X, y, X_val, y_val, \n","              learning_rate =1e-3, learning_rate_decay=0.95, \n","              reg=5e-6, num_iters=100, \n","              batch_size=200, it_verbose = 1, verbose=False):\n","        '''\n","        Train using SGD \n","        Input: \n","            X: nd array shape(N x D) \n","            y: vector of train label \n","            X_val: nd array shape( n_VAL , D) Use as validation set after each epoch \n","            y_val: vector of validation label \n","        '''\n","        N, D = X.shape\n","        N_val = X_val.shape[0]\n","        iteration_per_epoch = max(N/batch_size, 1)\n","        \n","        loss_hist = []\n","        train_acc_hist = []\n","        val_acc_hist = []\n","        \n","        for it in range(num_iters):\n","            sampling = np.random.choice(np.arange(N), batch_size, replace=False) \n","            X_batch = X[sampling]\n","            y_batch = y[sampling]\n","            \n","            #compute loss and gradients\n","            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n","            loss_hist.append(loss) \n","            \n","            #Update rule \n","            self.params['W1'] += (-1.0) * learning_rate * grads['W1']\n","            self.params['b1'] += (-1.0) * learning_rate * grads['b1']\n","            self.params['W2'] += (-1.0) * learning_rate * grads['W2']\n","            self.params['b2'] += (-1.0) * learning_rate * grads['b2']\n","            \n","            if(verbose and it%it_verbose==0):\n","                print('iteration: %d / %d | Loss: %f' % (it, num_iters, loss)) \n","            # Every epoch, check train and val accuracy and decay learning rate.\n","            if (it % iteration_per_epoch == 0):\n","                # Check accuracy\n","                train_acc = (self.predict(X_batch) == y_batch).mean()\n","                val_acc = (self.predict(X_val) == y_val).mean()\n","                train_acc_hist.append(train_acc)\n","                val_acc_hist.append(val_acc)\n","\n","                # Decay learning rate\n","                learning_rate *= learning_rate_decay\n","        return {\n","            'loss_hist':loss_hist,\n","            'train_acc_hist':train_acc_hist,\n","            'val_acc_hist':val_acc_hist\n","        }\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Use the trained weights of this two-layer network to predict labels for\n","        data points. For each data point we predict scores for each of the C\n","        classes, and assign each data point to the class with the highest score.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n","        classify.\n","\n","        Returns:\n","        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n","        the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n","        to have class c, where 0 <= c < C.\n","        \"\"\"\n","        y_pred = None\n","        relu = lambda x:np.maximum(0,x)\n","        # Unpack variables from the params dictionary\n","        W1, b1 = self.params['W1'], self.params['b1']\n","        W2, b2 = self.params['W2'], self.params['b2']\n","\n","        #Forward propagation though the network \n","        layer1 = relu(X.dot(W1) + b1)\n","        scores = layer1.dot(W2) + b2 #shape: (N x C)\n","        y_pred = np.argmax(scores, axis=1)\n","\n","        return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yu6uYRs5Y9vy","colab_type":"code","outputId":"3a371a5c-02fc-4892-f8f1-40ac562221d2","executionInfo":{"status":"ok","timestamp":1570941318345,"user_tz":-330,"elapsed":75267,"user":{"displayName":"Kolli Koushik","photoUrl":"","userId":"03448954236804754409"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["input_size = im_width * im_height * im_channel\n","hidden_size = 200\n","n_class = len(classes)\n","output_size = n_class \n","std = 1e-3 # size initialization parameter\n","\n","net = TwoLayerNet(input_size, hidden_size,output_size,std )\n","stats = net.train(X_dev, y_dev, X_val, y_val, \n","              learning_rate =1e-5, learning_rate_decay=0.95, \n","              reg=0.0, num_iters=70, \n","              batch_size=100, it_verbose = 10,verbose=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["iteration: 0 / 70 | Loss: 1.415624\n","iteration: 10 / 70 | Loss: 1.352437\n","iteration: 20 / 70 | Loss: 1.334587\n","iteration: 30 / 70 | Loss: 1.288995\n","iteration: 40 / 70 | Loss: 1.235902\n","iteration: 50 / 70 | Loss: 1.255482\n","iteration: 60 / 70 | Loss: 1.236090\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Nbc3OCaZEO_","colab_type":"code","outputId":"59a4c625-5ca6-41e1-f483-b935f691c9a9","executionInfo":{"status":"ok","timestamp":1570941609975,"user_tz":-330,"elapsed":2029,"user":{"displayName":"Kolli Koushik","photoUrl":"","userId":"03448954236804754409"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["print((net.predict(X_test)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 3 0 1 2 0 1 0 1 0 1 0 3 0 0 2 2 1\n"," 3 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 3 0 3 0 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n"," 1 1 1 0 0 1 2 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 1 2 2 2 2 2 2\n"," 2 2 1 1 0 0 1 1 1 1 3 0 1 1 1 1 3 1 0 1 3 1 3 1 1 3 1 1 1 0 1 1 1 3 1 1 0\n"," 1 3 1 3 1 3 1 0 0 1 3 1 0 0 2]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KwVEDKzDaZFm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}